<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Aidan Kierans - AI Safety Researcher</title>
    <style>
        body {
            font-family: system-ui, -apple-system, sans-serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
        }

        .header {
            margin-bottom: 2rem;
        }

        .profile-section {
            display: flex;
            gap: 2rem;
            align-items: flex-start;
            margin-bottom: 2rem;
        }

        .profile-image {
            width: 200px;
            height: 200px;
            object-fit: cover;
            border-radius: 4px;
        }

        h1 {
            color: #2c3e50;
            margin: 0;
        }

        h2 {
            color: #34495e;
            border-bottom: 2px solid #eee;
            padding-bottom: 0.5rem;
            margin-top: 2rem;
        }

        .contact-info {
            margin-top: 1rem;
        }

        .publication {
            margin-bottom: 1.5rem;
            padding-left: 1rem;
            border-left: 3px solid #eee;
        }

        .publication-title {
            font-weight: bold;
        }

        a {
            color: #3498db;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        .navigation {
            margin-bottom: 2rem;
        }

        .navigation a {
            margin-right: 1rem;
        }

        ul.highlights {
            padding-left: 1.2rem;
        }

        ul.highlights li {
            margin-bottom: 0.5rem;
        }

        @media (max-width: 600px) {
            .profile-section {
                flex-direction: column;
            }

            .profile-image {
                width: 150px;
                height: 150px;
            }
        }
    </style>
</head>
<body>
    <div class="navigation">
        <a href="#about">About</a>
        <a href="#publications">Publications</a>
        <a href="#experience">Experience</a>
        <a href="#recognition">Recognition</a>
        <a href="#media">Media</a>
    </div>

    <div class="header">
        <div class="profile-section">
            <img src="portrait-photo.jpg" alt="Profile Photo" class="profile-image">
            <div>
                <h1>Aidan Kierans</h1>
                <p>Ph.D. Student in Computer Science and Engineering<br>
                University of Connecticut</p>

                <div class="contact-info">
                    <p>aidan.kierans [at] uconn.edu<br>
                    <a href="https://www.linkedin.com/in/aidan-kierans/">LinkedIn</a> · <a href="https://drive.google.com/file/d/1d3VEzaYD7UUOeYLtzwj8KmfK-rUK9pvA/view?usp=sharing">CV</a></p>
                </div>
            </div>
        </div>
    </div>

    <section id="about">
        <h2>About</h2>
        <p>I am a Ph.D. student at the University of Connecticut (expected 2027), advised by Prof. Shiri Dori-Hacohen in the <a href="https://infothreats.cse.uconn.edu/">Reducing Information Ecosystem Threats (RIET) Lab</a>. My research focuses on what it takes for AI systems to reason well about moral issues.</p>

        <p>My earlier work on quantifying misalignment (AAAI-25) addressed preference divergence between agents. My recent position papers argue that alignment requires more than matching preferences—AI systems also need to follow appropriate reasoning norms. I'm now building knowledge graph infrastructure for philosophical argumentation to support this broader vision of moral reasoning for AI.</p>

        <p>Beyond research, I work on AI safety evaluation and policy. I have contributed to frontier model evaluations through the OpenAI Red Teaming Network and Nemesys Insights, and to governance discussions through the Wilson Center's Pathways to AI Policy program, as a Google Policy Fellow at CDT, and at NIST workshops. I also founded <a href="https://www.beacon-ai.club/">BEACON</a> and led the 2025 Machine Ethics and Reasoning Workshop.</p>
    </section>

    <section id="publications">
        <h2>Publications</h2>

        <div class="publication">
            <p class="publication-title">Position: Aligning AI Requires Automating Reasoning Norms</p>
            <p><u>Kierans, A.</u>, Ghosh, A., Dori-Hacohen, S.<br>
            Submitted to ICML 2026<br>
            [<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5399451">Preprint</a>]</p>
        </div>

        <div class="publication">
            <p class="publication-title">Position: Why LLMs Should Be Reasonably Morally Inconsistent</p>
            <p>Stenseke, J., <u>Kierans, A.</u>, Pres, I., Hadfield-Menell, D.<br>
            Submitted to ICML 2026<br>
            [<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5399451">Preprint</a>]</p>
        </div>

        <div class="publication">
            <p class="publication-title">Catastrophic Liability: Managing Systemic Risks in Frontier AI Development</p>
            <p><u>Kierans, A.</u>, Ritticher, K., Sonsayar, U., Ghosh, A.<br>
            TAIS 2025 and EAAMO 2025<br>
            [<a href="https://arxiv.org/abs/2505.00616">Preprint</a>]</p>
        </div>

        <div class="publication">
            <p class="publication-title">Quantifying Misalignment Between Agents: Towards a Sociotechnical Understanding of Alignment</p>
            <p><u>Kierans, A.</u>, Ghosh, A., Hazan, H., Dori-Hacohen, S.<br>
            AAAI 2025, Special Track on AI Alignment<br>
            [<a href="https://doi.org/10.1609/aaai.v39i26.34947">Paper</a>] [<a href="https://arxiv.org/abs/2406.04231">Preprint</a>]</p>
        </div>

        <div class="publication">
            <p class="publication-title">Benchmarked Ethics: A Roadmap to AI Alignment, Moral Knowledge, and Control</p>
            <p><u>Kierans, A.</u><br>
            AIES 2023<br>
            [<a href="https://doi.org/10.1145/3600211.3604764">Paper</a>]</p>
        </div>

        <div class="publication">
            <p class="publication-title">Bootstrap percolation via automated conjecturing</p>
            <p>Bushaw, N., Conka, B., Gupta, V., <u>Kierans, A.</u>, Lafayette, H., Larson, C., et al.<br>
            Ars Mathematica Contemporanea, 2023<br>
            [<a href="https://doi.org/10.26493/1855-3974.2340.a61">Paper</a>]</p>
        </div>
    </section>

    <section id="experience">
        <h2>Selected Experience</h2>
        <ul class="highlights">
            <li><strong>Graduate Research Assistant</strong>, UConn RIET Lab (2022–present) — AI alignment research</li>
            <li><strong>Independent Contractor</strong>, OpenAI Red Teaming Network (2024–present) — Evaluated frontier models including computer-using agents</li>
            <li><strong>Founder & President</strong>, Beneficial and Ethical AI at UConn (BEACON) (2024–2025) — Student group running AI safety fellowship cohorts</li>
            <li><strong>Teaching Assistant</strong>, ML Alignment & Theory Scholars (MATS) (2024–2025) — Facilitated AI safety strategy sessions</li>
            <li><strong>Google Policy Fellow</strong>, Center for Democracy & Technology (2023) — Supported Senate testimony on AI and human rights</li>
        </ul>
        <p style="margin-top: 1rem; font-size: 0.95em;">See <a href="https://drive.google.com/file/d/1d3VEzaYD7UUOeYLtzwj8KmfK-rUK9pvA/view?usp=sharing">CV</a> for complete history.</p>
    </section>

    <section id="recognition">
        <h2>Selected Recognition</h2>
        <ul class="highlights">
            <li><strong>Nemesys Insights Red Teaming Exercise</strong> — Biological Risk Category Winner (2025)</li>
            <li><strong>Wilson Center Pathways to AI Policy</strong> — Selected Participant (2024–2025)</li>
            <li><strong>ICLP Safe and Trustworthy AI Workshop</strong> — Best Poster Award (2023)</li>
            <li><strong>NeurIPS ML Safety Workshop</strong> — AI Risk Analysis Award (2022)</li>
        </ul>
    </section>

    <section id="media">
        <h2>Media</h2>
        <ul class="highlights">
            <li><a href="https://theconversation.com/getting-ais-working-toward-human-goals-study-shows-how-to-measure-misalignment-236050">The Conversation</a>: "Getting AIs working toward human goals — study shows how to measure misalignment"</li>
            <li><a href="https://dailycampus.com/2024/02/15/artificial-intelligence-poses-novel-social-threats-researchers-prepare-for-the-worst/">UConn Daily Campus</a>: "Artificial Intelligence poses novel social threats, researchers prepare for the worst"</li>
            <li><strong>Future of Life Institute</strong>: Invited talk for AI Existential Safety Community</li>
            <li><strong>UConn Center for Excellence in Teaching and Learning</strong>: Led "mAI dAI" seminar series on AI alignment and malicious misuse for university instructional staff</li>
        </ul>
    </section>

    <footer style="margin-top: 3rem; padding-top: 1rem; border-top: 1px solid #eee; color: #666; font-size: 0.9em;">
        <p>Last updated: February 2026</p>
    </footer>
</body>
</html>
