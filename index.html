<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Aidan Kierans - AI Safety Researcher</title>
    <style>
        body {
            font-family: system-ui, -apple-system, sans-serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
        }
        
        .header {
            margin-bottom: 2rem;
        }
        
        .profile-section {
            display: flex;
            gap: 2rem;
            align-items: flex-start;
            margin-bottom: 2rem;
        }
        
        .profile-image {
            width: 200px;
            height: 200px;
            object-fit: cover;
            border-radius: 4px;
        }
        
        h1 {
            color: #2c3e50;
            margin: 0;
        }
        
        h2 {
            color: #34495e;
            border-bottom: 2px solid #eee;
            padding-bottom: 0.5rem;
            margin-top: 2rem;
        }
        
        .contact-info {
            margin-top: 1rem;
        }
        
        .publication {
            margin-bottom: 1.5rem;
            padding-left: 1rem;
            border-left: 3px solid #eee;
        }
        
        .publication-title {
            font-weight: bold;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        .navigation {
            margin-bottom: 2rem;
        }
        
        .navigation a {
            margin-right: 1rem;
        }
        
        .award {
            margin-bottom: 1rem;
        }
        
        .experience {
            margin-bottom: 1.5rem;
        }
        
        .conference {
            margin-bottom: 1rem;
            padding-left: 1rem;
        }
        
        .project {
            margin-bottom: 1.5rem;
            padding-left: 1rem;
            border-left: 3px solid #eee;
        }
        
        @media (max-width: 600px) {
            .profile-section {
                flex-direction: column;
            }
            
            .profile-image {
                width: 150px;
                height: 150px;
            }
        }
    </style>
</head>
<body>
    <div class="navigation">
        <a href="#about">About</a>
        <a href="#education">Education</a>
        <a href="#experience">Experience</a>
        <a href="#publications">Publications</a>
        <a href="#conferences">Conferences</a>
        <a href="#projects">Projects</a>
        <a href="#awards">Awards</a>
        <a href="#teaching">Teaching</a>
        <a href="#media">Media</a>
        <a href="#contact">Contact</a>
    </div>

    <div class="header">
        <div class="profile-section">
            <img src="portrait-photo.jpg" alt="Profile Photo" class="profile-image">
            <div>
                <h1>Aidan Kierans</h1>
                <p>Ph.D. Student in Computer Science and Engineering<br>
                University of Connecticut</p>
                
                <div class="contact-info">
                    <p>ðŸ“§ aidan.kierans [at] uconn.edu<br>
                    <a href="https://www.linkedin.com/in/aidan-kierans/">LinkedIn</a></p>
                </div>
            </div>
        </div>
    </div>

    <section id="about">
        <h2>About</h2>
        <p>I am a Ph.D. student in Computer Science and Engineering at the University of Connecticut, focusing on AI alignment and technical governance. My research has been recognized with awards from workshops at NeurIPS and ICLP, and I have published AAAI on quantifying misalignment.</p>
        
        <p>I am advised by Prof. Shiri Dori-Hacohen in the <a href="https://infothreats.cse.uconn.edu/">Reducing Information Ecosystem Threats (RIET) Lab</a>. I founded <a href="https://www.beacon-ai.club/">Beneficial and Ethical AI at UConn (BEACON)</a>, which has grown to serve over 30 students across six concurrent fellowship cohorts.</p>
        
        <p>My work spans technical AI safety research, policy development, and risk assessment. I contribute to frontier AI safety through the OpenAI Red Teaming Network, where I have identified vulnerabilities in early-stage computer-using agent systems. I also serve on the Leadership Council of the Collegiate Coalition for AI Policy, coordinating AI safety advocacy across university student groups nationwide.</p>
    </section>

    <section id="publications">
        <h2>Publications</h2>
        <div class="publication">
            <p class="publication-title">Catastrophic Liability: Managing Systemic Risks in Frontier AI Development</p>
            <p>Kierans, A., Ritticher, K., Sonsayar, U., Ghosh, A.<br>
            Presented at Technical AI Safety Conference (TAIS) 2025. Submitted to Artificial Intelligence, Ethics, and Society (AIES) 2025. Accepted for poster presentation at ACM EAAMO 2025.<br>
            [<a href="https://www.tais2025.cc/proceedings/kierans-safety-documentation-and-liability">Poster</a>] [<a href="https://arxiv.org/abs/2505.00616">Preprint</a>]</p>
        </div>

        <div class="publication">
            <p class="publication-title">ReNorM: Aligning AI Requires Automating Reasoning Norms</p>
            <p>Kierans, A.<br>
            Submitted to International Association for Statutes, Epistemology, and Applications of Inferences (IASEAI) 2026<br>
            [<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5399451">Preprint</a>]</p>
        </div>

        <div class="publication">
            <p class="publication-title">Quantifying Misalignment Between Agents: Towards a Sociotechnical Understanding of Alignment</p>
            <p>Kierans, A., Ghosh, A., Hazan, H., & Dori-Hacohen, S.<br>
            Proceedings of the AAAI Conference on Artificial Intelligence (AAAI 2025), Special Track on AI Alignment<br>
            [<a href="https://doi.org/10.1609/aaai.v39i26.34947">Paper</a>] [<a href="https://arxiv.org/abs/2406.04231">Preprint</a>]</p>
        </div>

        <div class="publication">
            <p class="publication-title">Benchmarked Ethics: A Roadmap to AI Alignment, Moral Knowledge, and Control</p>
            <p>Kierans, A.<br>
            Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society (pp. 964-965)<br>
            [<a href="https://doi.org/10.1145/3600211.3604764">Paper</a>]</p>
        </div>

        <div class="publication">
            <p class="publication-title">Quantifying Misalignment Between Agents</p>
            <p>Kierans, A., Hazan, H., Dori-Hacohen, S.<br>
            NeurIPS ML Safety Workshop 2022 (Won AI Risk Analysis Award)<br>
            [<a href="https://arxiv.org/abs/2406.04231">Paper</a>]</p>
        </div>

        <div class="publication">
            <p class="publication-title">Bootstrap percolation via automated conjecturing</p>
            <p>Bushaw, N., Conka, B., Gupta, V., Kierans, A., Lafayette, H., Larson, C., McCall, K., Mulyar, A., Sullivan, C., Taylor, S., Wainright, E., Wilson, E., Wu, G., Loeb, S.<br>
            Ars Mathematica Contemporanea, 23(3), P3-06 (2023)<br>
            [<a href="https://doi.org/10.26493/1855-3974.2340.a61">Paper</a>]</p>
        </div>
    </section>

    <section id="education">
        <h2>Education</h2>
        <ul>
            <li><strong>Ph.D. in Computer Science and Engineering</strong><br>
                University of Connecticut (Expected May 2027)<br>
                Research focused on AI alignment and technical governance</li>
            <li><strong>M.S. in Computer Science and Engineering</strong> (coursework equivalency)<br>
                University of Connecticut (February 2025)</li>
            <li><strong>BS in Computer Science, BA in Philosophy</strong> (cum laude)<br>
                Virginia Commonwealth University (May 2021)<br>
                BS: Concentration in Data Science | BA: Concentration in Philosophy and Science | Minor in Mathematics</li>
        </ul>
    </section>

    <section id="experience">
        <h2>Selected Experience</h2>
        
        <div class="experience">
            <p><strong>Co-Founder | Leadership Council</strong>, Collegiate Coalition for AI Policy (CCAP) (March 2025 â€“ Present)<br>
            Co-founding non-profit organization establishing pathways for students into AI policy advocacy.</p>
        </div>

        <div class="experience">
            <p><strong>Independent Contractor</strong>, OpenAI Red Teaming Network (Jan 2024 â€“ Present)<br>
            Participating in red teaming efforts to assess risks and safety profiles of OpenAI models. Red-teamed OpenAI's Computer-Using Agent (CUA) model, "Operator", at early development stages, directly informing risk assessments and System Card outputs.</p>
        </div>

        <div class="experience">
            <p><strong>Graduate Research Assistant</strong>, UConn Reducing Information Ecosystem Threats (RIET) Lab (June 2022 â€“ Present)<br>
            Published first-author papers on quantifying misalignment, winning the NeurIPS ML Safety Workshop AI Risk Analysis Award (2022), ICLP Best Poster Award (2023), and ultimately a top conference publication (AAAI-25 AI Alignment Track). Served on program committees for 6+ workshops and conferences, reviewing 10+ papers. Currently developing knowledge graph infrastructure for philosophy publications to support research on automating moral reasoning.</p>
        </div>

        <div class="experience">
            <p><strong>Founder & President</strong>, Beneficial and Ethical AI at UConn (BEACON) (Feb 2024 â€“ May 2025)<br>
            Built fellowship program from zero to 30+ students across six concurrent cohorts with 80%+ overall retention and 100% retention after week 2. Scaled organizing team to 6 graduate and undergraduate students. Two undergraduate fellows launched independent research projects; one joined RIET Lab and co-presented at CHAI-24. Secured Open Philanthropy University Group Fellowship funding.</p>
        </div>

        <div class="experience">
            <p><strong>Graduate Teaching Assistant</strong>, ENGR 1195: AI Literacy, University of Connecticut (Aug 2025 â€“ Dec 2025)<br>
            Analyzed engagement patterns across 500 students from engineering and other disciplines, identifying key factors influencing student interactions with AI. Generated statistical reports on student responses for Provost assessment and course improvement recommendations.</p>
        </div>

        <div class="experience">
            <p><strong>Teaching Assistant & Contractor</strong>, ML Alignment & Theory Scholars (MATS) (Summer 2024 & 2025)<br>
            Facilitated weekly "AI Safety Strategy" curriculum for MATS cohort 6.0, guiding emerging researchers through threat model analysis and research prioritization. Evaluated scholars' research proposals for technical feasibility and alignment with AI safety priorities.</p>
        </div>

        <div class="experience">
            <p><strong>Google Policy Fellow</strong>, Center for Democracy & Technology (CDT) (June 2023 â€“ Aug 2023)<br>
            Ensured technical accuracy of CEO testimony for Senate committee hearing on AI and human rights. Authored briefing materials on AI open-sourcing risks and participated in policy discussions with Meta and Google policy teams. Developed automated data extraction pipeline and created comprehensive "AI Policy Coordination Tracker" database cataloging 150+ CDT policy publications, reducing future data collection time by 90%.</p>
        </div>

        <div class="experience">
            <p><strong>Graduate Entrepreneurship Fellow</strong>, UConn Engineering Entrepreneurship Hub (Aug 2024 â€“ Jul 2025)<br>
            Conducted 15+ interviews with senior technologists, policy counsel, and government officialsâ€”including the Assistant Director of AI R&D at the White House OSTPâ€”to identify gaps in AI safety auditing infrastructure. Synthesized findings into recommendations for transparency standards, reporting requirements, and whistleblower protections in AI development.</p>
        </div>

        <div class="experience">
            <p><strong>Lead Data Scientist</strong>, OpenPrinciples (Jan 2022 â€“ Dec 2022)<br>
            Redesigned semantic search pipeline that doubled recommendation accuracy from baseline. Fine-tuned GPT-3 on domain-specific dataset to generate contextually relevant life principle recommendations. Developed cosine similarity-based extraction system to automatically identify high-value quotes from unstructured text.</p>
        </div>
    </section>

    <section id="conferences">
        <h2>Selected Conferences & Presentations</h2>
        <div class="conference">
            <p>â€¢ <strong>ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization (EAAMO 2025)</strong>: Poster presentation on "Catastrophic Liability: Managing Systemic Risks in Frontier AI Development"</p>
            <p>â€¢ <strong>AI Law and Policy Workshop (Vista Institute)</strong>: Selected participant in Washington, DC workshop on AI governance, cybersecurity, and biosecurity law</p>
            <p>â€¢ <strong>NIST AISIC Workshop on AI Standards Zero Drafts Pilot Project</strong>: Provided technical feedback on early-stage standards for AI evaluation and system documentation</p>
            <p>â€¢ <strong>Congressional Exhibition on Advanced AI</strong>: Led team that designed and presented AI scheming demonstration at Capitol Hill event organized by the Center for AI Policy</p>
            <p>â€¢ <strong>Technical Innovations for AI Policy Conference</strong>: Presented AI scheming demonstration at inaugural Washington, D.C. conference hosted by FAR.AI</p>
            <p>â€¢ <strong>Technical AI Safety Conference 2025 (TAIS)</strong>: Presented poster on "Catastrophic Liability: Managing Systemic Risks in Frontier AI Development"</p>
            <p>â€¢ <strong>AAAI Conference on Artificial Intelligence (AAAI-25)</strong>: Published full paper in Special Track on AI Alignment; reviewed 4 papers as Program Committee member</p>
            <p>â€¢ <strong>Center for Human-Compatible AI Workshop (CHAI 2024)</strong>: Presented first-author poster on "Quantifying Misalignment Between Agents"; co-presented "Reinforcement Learning from Oracle Feedback"</p>
            <p>â€¢ <strong>The Safe and Trustworthy AI Workshop (ICLP 2023)</strong>: Won Best Poster Award for "Quantifying Misalignment Between Agents"; served as Junior Program Committee member</p>
            <p>â€¢ <strong>NeurIPS ML Safety Workshop 2022</strong>: Won AI Risk Analysis Award for "Quantifying Misalignment Between Agents"</p>
        </div>
    </section>

    <section id="projects">
        <h2>Notable Projects</h2>
        
        <div class="project">
            <p><strong>Machine Ethics and Reasoning (MERe) Workshop</strong> (July 2025)<br>
            Led 4-person organizing team for interdisciplinary virtual workshop attracting 75+ applicants from philosophy and computer science. Delivered plenary talk on "Computational Approaches to Moral Reasoning." Managed parallel OCR processing for 100K PDFs and technical requirements for collaborative annotation interface.</p>
        </div>
        
        <div class="project">
            <p><strong>Moral Knowledge Graph for AI Alignment</strong> (Ongoing)<br>
            Creating the first large-scale dataset of philosophical arguments for machine understanding of moral reasoning. Building knowledge graph infrastructure to support thesis research on automating reasoning norms.</p>
        </div>
        
        <div class="project">
            <p><strong>Safety Assurance Index (SAI)</strong> (2025)<br>
            Developed white paper proposing open-source standards for AI safety documentation and verification. Won 3rd place at Apart Research AI Safety & Assurance Startup Hackathon.</p>
        </div>
        
        <div class="project">
            <p><strong>AI Safety Learning Community</strong> (Jan 2025 â€“ May 2025)<br>
            Designed and delivered month-long AI safety seminar series for UConn faculty and staff, with support from UConn's Center for Excellence in Teaching and Learning. Seminar "AI Safety Literacy: From Awareness to Action" focused on understanding and addressing AI safety challenges in educational contexts.</p>
        </div>

        <div class="project">
            <p><strong>Congressional Exhibition on Advanced AI</strong> (2024)<br>
            Proposed, designed, and led small team to create AI risk demonstration presented on Capitol Hill at Center for AI Policy event. Demonstration illustrated potential for AI scheming behavior to congressional staff and policymakers.</p>
        </div>
    </section>

    <section id="awards">
        <h2>Selected Awards and Recognition</h2>
        <div class="award">
            <p>â€¢ <strong>AI Safety & Assurance Startup Hackathon (Apart Research) 3rd Place Prize</strong> (2025)</p>
            <p>â€¢ <strong>Open Philanthropy University Group Fellowship</strong> for BEACON (2024-2025)</p>
            <p>â€¢ <strong>Selected Participant in the Wilson Center's Pathways to AI Policy program</strong> (2024-2025)</p>
            <p>â€¢ <strong>UConn Engineering Entrepreneurship Hub Graduate Entrepreneurship Fellow</strong> (2024-2025)</p>
            <p>â€¢ <strong>The Safe and Trustworthy AI Workshop (ICLP 2023) Best Poster Award</strong></p>
            <p>â€¢ <strong>UConn Computer Science and Engineering Predoctoral Fellowship</strong> (2023)</p>
            <p>â€¢ <strong>NeurIPS ML Safety Workshop AI Risk Analysis Award</strong> (2022)</p>
            <p>â€¢ <strong>Vitalik Buterin PhD Fellowship in AI Existential Safety</strong> (2022 finalist)</p>
            <p>â€¢ <strong>UConn School of Engineering Next Gen Scholar GE Graduate Fellowship for Innovation</strong> (2021-2022)</p>
            <p>â€¢ <strong>Member of the Future of Life Institute's AI Safety Community Researchers</strong></p>
        </div>
    </section>

    <section id="teaching">
        <h2>Teaching and Communication</h2>
        
        <div class="experience">
            <p><strong>Instructor Support Volunteer</strong>, UConn Center for Excellence in Teaching and Learning (Jan 2025 â€“ May 2025)<br>
            Designed and taught month-long "AI Safety Literacy: From Awareness to Action" seminar series for UConn faculty and staff. Convened panel discussion "Beneficial, Ethical AI at UConn: a student-led conversation," earning letter of recognition from CETL. Provided consultation on AI integration in educational contexts through drop-in office hours.</p>
        </div>
        
        <div class="experience">
            <p><strong>Fellowship Facilitator</strong>, Beneficial and Ethical AI at UConn (BEACON) (Jan 2024 â€“ Dec 2024)<br>
            Facilitated Technical AI Safety Fellowship and AI Policy Fellowship for multiple cohorts, guiding students through structured curricula introducing AI safety research and governance. Mentored undergraduate research project on bias reduction; advised undergraduate fellow who subsequently joined RIET Lab and collaborated on CHAI-24 poster presentation.</p>
        </div>
        
        <div class="experience">
            <p><strong>Teaching Assistant</strong>, ML Alignment & Theory Scholars (MATS) (Jun 2024 â€“ Aug 2024)<br>
            Led weekly "AI Safety Strategy" sessions for MATS 6.0, guiding emerging scholars through curated readings on AI threat models and research prioritization. Evaluated research proposals for threat model analysis and technical feasibility.</p>
        </div>
    </section>

    <section id="media">
        <h2>Media and Outreach</h2>
        <div class="conference">
            <p>â€¢ <strong>The Conversation</strong>: Published article "<a href="https://theconversation.com/getting-ais-working-toward-human-goals-study-shows-how-to-measure-misalignment-236050">Getting AIs working toward human goals â€“ study shows how to measure misalignment</a>" communicating research findings to general audience</p>
            <p>â€¢ <strong>UConn Daily Campus</strong>: Featured in "<a href="https://dailycampus.com/2024/02/15/artificial-intelligence-poses-novel-social-threats-researchers-prepare-for-the-worst/">Artificial Intelligence poses novel social threats, researchers prepare for the worst</a>" discussing AI safety research</p>
            <p>â€¢ <strong>Future of Life Institute</strong>: Presented invited talk "Quantifying Misalignment Between Agents: Towards a Sociotechnical Understanding of Alignment" for AI Existential Safety Community</p>
            <p>â€¢ <strong>UConn Center for Excellence in Teaching and Learning</strong>: Led "mAI dAI" seminar series on AI alignment and malicious misuse for university instructional staff</p>
        </div>
    </section>

    <section id="contact">
        <h2>Contact</h2>
        <p>Feel free to reach out at aidan.kierans [at] uconn.edu or connect with me on <a href="https://www.linkedin.com/in/aidan-kierans/">LinkedIn</a>.</p>
    </section>

    <footer style="margin-top: 3rem; padding-top: 1rem; border-top: 1px solid #eee; color: #666; font-size: 0.9em;">
        <p>Last updated: October 2025</p>
    </footer>
</body>
</html>
